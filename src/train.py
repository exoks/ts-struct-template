#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£¶‚£¥‚£∂‚£æ‚£ø‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢†‚°∂‚†ª‚†õ‚†ü‚†ã‚†â‚†Ä‚†à‚†§‚†¥‚†∂‚†∂‚¢æ‚£ø‚£ø‚£ø‚£∑‚£¶‚†Ñ‚†Ä‚†Ä‚†Ä                ìêì  train.py ìêî           
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†î‚†ã‚†Ä‚†Ä‚†§‚†í‚†í‚¢≤‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£§‚£¨‚£Ω‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä
#  ‚†Ä‚†Ä‚†Ä‚£Ä‚£é‚¢§‚£∂‚£æ‚†Ö‚†Ä‚†Ä‚¢Ä‚°§‚†è‚†Ä‚†Ä‚†Ä‚††‚£Ñ‚£à‚°ô‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚†Ä       Eng: oezzaou <oussama.ezzaou@gmail.com>
#  ‚¢Ä‚†î‚†â‚†Ä‚†ä‚†ø‚†ø‚£ø‚†Ç‚††‚†¢‚£§‚†§‚£§‚£º‚£ø‚£∂‚£∂‚£§‚£ù‚£ª‚£∑‚£¶‚£ç‚°ª‚£ø‚£ø‚£ø‚£ø‚°Ä
#  ‚¢æ‚£æ‚£Ü‚£§‚£§‚£Ñ‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á
#  ‚†Ä‚†à‚¢ã‚¢π‚†ã‚†â‚†ô‚¢¶‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á       Created: 2025/07/24 16:43:38 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†ë‚†Ä‚†Ä‚†Ä‚†à‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†á       Updated: 2025/08/29 10:48:12 by oezzaou
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°á‚†Ä‚†Ä‚¢Ä‚£æ‚£ø‚£ø‚†ø‚†ü‚†õ‚†ã‚†õ‚¢ø‚£ø‚£ø‚†ª‚£ø‚£ø‚£ø‚£ø‚°ø‚†Ä
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚†á‚†Ä‚¢†‚£ø‚£ü‚£≠‚£§‚£∂‚£¶‚£Ñ‚°Ä‚†Ä‚†Ä‚†à‚†ª‚†Ä‚†ò‚£ø‚£ø‚£ø‚†á‚†Ä
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†±‚†§‚†ä‚†Ä‚¢Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£ø‚†è‚†Ä‚†Ä                             ìÜ©‚ôïìÜ™
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ñ‚†Ä‚†Ä‚†Ä‚†ò‚¢ß‚°Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ê‚†ã‚†Ä‚†Ä‚†Ä                     ìÑÇ oussama ezzaouìÜÉ
#  ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ñ‚£Ä‚°Ä‚†∏‚†ì‚†Ä‚†Ä‚†Ä‚††‚†ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä


# ===[ Imports: ]==============================================================
from data_loader import load_raw_data, save_processed_data, train_test_split
from preprocessing import preprocess_data
from evaluate import evaluate_model
from utils.logger import getLogger
import models
import config


# ===[ main: ]=================================================================
def main():
    logger = getLogger(__name__)
    try:
        # 1. Loading the raw dataset
        data_raw = load_raw_data()
        # 2. Data Preprocessing
        data = preprocess_data(data_raw)
        # 3. Save processed data
        save_processed_data(data)
        # 4. train-test split
        train_set, test_set = train_test_split(data, feature=None, period=1)
        # 5. Building Model (build the model in `models.py`)
        model = models.build_holt_winters(train_set, config.params)
        # 6. Evaluate the Model
        forecasts, metrics = evaluate_model(model, train_set, plot=True)

        # =================================================
        # 1. basic exploration for the data
        print("==================[ Data ]================")
        print(data.head())
        # 2. display forecasts/predictions
        print("================[ Forecasts ]=============")
        # print(forecasts)
        # 3. displaying metrics
        print("=================[ Metrics ]==============")
        # for metric, acurancy in metrics.items():
        #     print(f"{metric}: {acurancy: .2f}")
        # =================================================
    except Exception as msg:
        logger.error(msg)


if __name__ == '__main__':
    main()


# ml-project/
# ‚îú‚îÄ‚îÄ data/
# ‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Original, immutable data dump
# ‚îÇ   ‚îú‚îÄ‚îÄ processed/            # Cleaned data for modeling
# ‚îÇ   ‚îî‚îÄ‚îÄ external/             # Any 3rd-party or API data
# ‚îú‚îÄ‚îÄ notebooks/                # Jupyter notebooks for exploration & prototyping
# ‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb          # You do your initial data exploration here
# |   |                           (visualizations, correlations, missing values,
# |   |                           etc)
# ‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_engineering.ipynb
# ‚îÇ   ‚îî‚îÄ‚îÄ 03_modeling.ipynb
# ‚îú‚îÄ‚îÄ src/                      # Core Python source code
# ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
# ‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Global config variables
# ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py        # Load and save data
# ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py      # Feature engineering, data cleaning
# ‚îÇ   ‚îú‚îÄ‚îÄ train.py              # Model training
# ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py           # Evaluation metrics and visualization
# ‚îÇ   ‚îî‚îÄ‚îÄ model.py              # Model class, pipelines
# ‚îú‚îÄ‚îÄ models/                   # Trained model binaries (e.g., .pkl, .h5)
# ‚îÇ   ‚îî‚îÄ‚îÄ final_model.pkl
# ‚îú‚îÄ‚îÄ reports/                  # Results, plots, performance metrics
# ‚îÇ   ‚îú‚îÄ‚îÄ figures/
# ‚îÇ   ‚îî‚îÄ‚îÄ metrics.txt
# ‚îú‚îÄ‚îÄ tests/                    # Unit and integration tests
# ‚îÇ   ‚îî‚îÄ‚îÄ test_model.py
# ‚îú‚îÄ‚îÄ scripts/                  # CLI scripts (optional)
# ‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py
# ‚îú‚îÄ‚îÄ .gitignore
# ‚îú‚îÄ‚îÄ README.md
# ‚îú‚îÄ‚îÄ requirements.txt
# ‚îú‚îÄ‚îÄ setup.py                  # Optional for packaging
# ‚îî‚îÄ‚îÄ LICENSE


# INFO:------------------------------------------------------------------------
# > 'train.py': The 'conductor' of the orchestar, it does not do everything
#   itself, but 'corrdinates': data, model preprocessing, evaluation
# > 'model.py' is to 'define and return your machine learning model' or
#   'pipeline'.
#   - It actes like a factory: you give it parameters (from 'config.py'
#     and it gives you back a ready-to-train model object
# > 'config.py': This file stores all the 'global settings', 'parameters', 'and
#   file paths' that your project needs.
# -----------------------------------------------------------------------------
